{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torchvision import transforms\n",
    "from astropy.io import fits \n",
    "from skimage.transform import resize\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "#matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from weighting import weighting\n",
    "\n",
    "torch.cuda.benchmark=True\n",
    "\n",
    "IMG_PATH = \"E:/Documents/Python_Scripts/CNN/TRAINING/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = torch.nn.Sequential(\n",
    "            \n",
    "            torch.nn.Conv2d(1,64,5,padding=2), # 1 input, 32 out, filter size = 5x5, 2 block outer padding\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64,128,5,padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.Conv2d(128,256,5,padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(256),\n",
    "            torch.nn.MaxPool2d(2))\n",
    " \n",
    "        self.classifier = torch.nn.Sequential(\n",
    "\t\t    torch.nn.Dropout(0.25),\n",
    "            torch.nn.Linear(256*16*16,256), # Fully connected layer \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.25),\n",
    "            torch.nn.Linear(256,10))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        features = self.feature_extractor(x)\n",
    "        output = self.classifier(features.view(int(x.size()[0]),-1))\n",
    "        output= torch.nn.functional.log_softmax(output,dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 64, 5, padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 128, 5, padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.Conv2d(128, 256, 5, padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(256, 512, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(512),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.Conv2d(512, 512, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(512, 512, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(512),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.Conv2d(512, 256, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(256, 256, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(256),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.Conv2d(256, 128, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(128, 128, 3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.MaxPool2d(2))\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(0.25),\n",
    "            torch.nn.Linear(128 * 2 * 2, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.25),\n",
    "            torch.nn.Linear(256, 10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        output = self.classifier(features.view(int(x.size()[0]), -1))\n",
    "        output= torch.nn.functional.log_softmax(output,dim=1) # Give results using softmax\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PIL_transform(_img):\n",
    "    _img[_img != _img] = 0\n",
    "    _img -= _img.min()\n",
    "    _img *= 255./_img.max()\n",
    "    _img = _img.astype(np.uint8)\n",
    "    return _img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "IMG_EXTENSIONS = [\n",
    "    \".fits\"\n",
    "]\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "\n",
    "def make_dataset(dir):\n",
    "    images = []\n",
    "    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
    "    for root, _, fnames in sorted(os.walk(dir)):\n",
    "        for fname in fnames:\n",
    "            if is_image_file(fname):\n",
    "                path = os.path.join(root, fname)\n",
    "                images.append(path)\n",
    "    return images\n",
    "\n",
    "\n",
    "def default_fits_loader(file_name: str, img_size: tuple, slice_index):\n",
    "    file = fits.open(file_name)\n",
    "    _data = file[1].data\n",
    "    _data = resize(_data[slice_index], img_size)\n",
    "    _label = file[0].header['LABEL']\n",
    "    file.close()\n",
    "\n",
    "    if len(_data.shape) < 3:\n",
    "        _data = _data.reshape((*_data.shape, 1))\n",
    "    \n",
    "    return _data, _label\n",
    "\n",
    "\n",
    "class FITSCubeDataset(data.Dataset):\n",
    "    def __init__(self, data_path, cube_length, transforms, img_size):\n",
    "        self.data_path = data_path\n",
    "        self.transforms = transforms\n",
    "        self.img_size = img_size\n",
    "        self.cube_length = cube_length\n",
    "        self.img_files = make_dataset(data_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        cube_index = index // self.cube_length\n",
    "        slice_index = index % self.cube_length\n",
    "        _img, _label = default_fits_loader(self.img_files[cube_index], self.img_size, slice_index)\n",
    "        _img[_img != _img] = 0\n",
    "        _img = PIL_transform(_img)\n",
    "        if self.transforms is not None:\n",
    "            _data = (self.transforms(_img), _label)\n",
    "        #else:\n",
    "        #    _data = (_img, _label)\n",
    "            \n",
    "        return _data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)*self.cube_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = \"E:/Documents/Python_Scripts/CNN/TRAINING/\"\n",
    "\n",
    "def plot_accuracy(accuracies, val_acc, epochs, filename):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca()\n",
    "    ax.set_xlim(0, max(epochs))\n",
    "    ax.set_ylim(0, 100)\n",
    "    plt.plot(epochs, accuracies,'b',label='Training Accuracy',zorder=1)\n",
    "    plt.plot(epochs, val_acc,'purple',label='Validation Accuracy',zorder=0)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Training Epoch')\n",
    "    plt.legend(loc='best',fontsize='small')\n",
    "    fig.savefig(IMG_PATH+filename, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, initial_lr, num_epochs):\n",
    "    decay = initial_lr / num_epochs\n",
    "    lr = initial_lr - decay*epoch\n",
    "    print(\"Set LR to %f\" % lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: torch.nn.Module, \n",
    "          transforms, \n",
    "          data_path=\"E:/Documents/Python_Scripts/CNN/TRAINING/EXAMPLES/\", \n",
    "          val_path=\"E:/Documents/Python_Scripts/CNN/TRAINING/EXAMPLES/\",  \n",
    "          num_epochs=50, \n",
    "          batch_size=32, \n",
    "          verbose=True,\n",
    "          cube_length=640, img_size=(64, 64), \n",
    "          loss=torch.nn.CrossEntropyLoss(), \n",
    "          lr_schedule=True, initial_lr=1e-3, suffix=\"\"):\n",
    "\n",
    "    data_path = os.path.abspath(data_path)\n",
    "    val_path = os.path.abspath(val_path)\n",
    "\t\n",
    "    model = model.train()\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = model.to(device).to(torch.float)\n",
    "    start = time.time()\n",
    "    \n",
    "    print('Creating sampling weight array')\n",
    "    train_loader = DataLoader(FITSCubeDataset(data_path, cube_length, transforms, img_size), \n",
    "                              batch_size=6399, shuffle=False)\n",
    "    dataiter = iter(train_loader)\n",
    "    dummy_labels = []\n",
    "    for idx, (batch, target) in enumerate(tqdm(train_loader)):\n",
    "        dummy_labels.append(np.array(target.numpy()))\n",
    "    dummy_labels = np.hstack(dummy_labels)\n",
    "    print(len(dummy_labels))\n",
    "    print('Number of labels=',len(set(dummy_labels)))\n",
    "    weights = weighting(dummy_labels)\n",
    "    sampler = WeightedRandomSampler(weights, len(weights))\n",
    "    end = time.time()\n",
    "    print('Weights Created in %.2gs'%(end-start))\n",
    "    #batch size was 10*640?\n",
    "    start = time.time()\n",
    "    val_loader = DataLoader(FITSCubeDataset(val_path, cube_length, transforms, img_size), \n",
    "                            batch_size=6399, shuffle=False)\n",
    "    dataiter = iter(val_loader)\n",
    "    dummy_val_labels = []\n",
    "    for idx, (batch, target) in enumerate(tqdm(val_loader)):\n",
    "        dummy_val_labels.append(np.array(target.numpy()))\n",
    "    dummy_val_labels = np.hstack(dummy_val_labels)\n",
    "    print(len(dummy_val_labels))\n",
    "    print('Number of labels=',len(set(dummy_val_labels)))\n",
    "    val_weights = weighting(dummy_val_labels)\n",
    "    val_sampler = WeightedRandomSampler(val_weights, len(val_weights))\n",
    "    end = time.time()\n",
    "    print('Validation weights Created in %.2gs'%(end-start))\n",
    "    \n",
    "    loader = DataLoader(FITSCubeDataset(data_path, cube_length, transforms, img_size), \n",
    "                        batch_size, shuffle=False, sampler=sampler)\n",
    "    validation_loader = DataLoader(FITSCubeDataset(data_path, cube_length, transforms, img_size), \n",
    "                                   batch_size, shuffle=False, sampler=val_sampler)   \n",
    "    \n",
    "    optim = torch.optim.Adam(model.parameters(), initial_lr)\n",
    "\t\n",
    "    accuracies, val_accuracies, epochs = [0], [0], [0]\n",
    "\t\n",
    "    for i in range(num_epochs):\n",
    "        print(\"Epoch %d of %d\" % (i+1, num_epochs))\n",
    "        _accuracies = []\n",
    "        _val_accuracies = []\n",
    "        model.train(True)\n",
    "        for idx, (batch, target) in enumerate(tqdm(loader)):\n",
    "            batch = batch.to(device).to(torch.float)\n",
    "            if isinstance(loss, torch.nn.CrossEntropyLoss):\n",
    "                target = target.to(device).to(torch.long)\n",
    "            else:\n",
    "                target = target.to(device).to(torch.float)\n",
    "            pred = model(batch)\n",
    "\n",
    "            loss_value = loss(pred, target)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optim.step()\n",
    "\n",
    "            pred_npy = pred.detach().cpu().numpy()\n",
    "            target_npy = target.detach().cpu().numpy()\n",
    "\n",
    "            if isinstance(loss, torch.nn.CrossEntropyLoss):\n",
    "                pred_npy = np.argmax(pred_npy, axis=1) \n",
    "                \n",
    "            ###Change the error metric here###\n",
    "\n",
    "            pred_int = np.round(pred_npy).astype(np.uint8).reshape(-1)\n",
    "            target_npy = target_npy.astype(np.uint8).reshape(-1)\n",
    "\n",
    "            _accuracies.append(accuracy_score(target_npy, pred_int)*100)\n",
    "            \n",
    "        epochs.append(i+1)\n",
    "\n",
    "        mean_accuracy = sum(_accuracies)/len(_accuracies)\n",
    "        accuracies.append(mean_accuracy)\n",
    "\n",
    "        print(\"Mean accuracy: %f\" % mean_accuracy)\n",
    "        \n",
    "        model.train(False)\n",
    "\n",
    "        for idx, (batch, target) in enumerate(tqdm(validation_loader)):\n",
    "            batch = batch.to(device).to(torch.float)\n",
    "            if isinstance(loss, torch.nn.CrossEntropyLoss):\n",
    "                target = target.to(device).to(torch.long)\n",
    "            else:\n",
    "                target = target.to(device).to(torch.float)\n",
    "            pred = model(batch)\n",
    "\n",
    "            loss_value = loss(pred, target)\n",
    "\n",
    "            pred_npy = pred.detach().cpu().numpy()\n",
    "            target_npy = target.detach().cpu().numpy()\n",
    "\n",
    "            if isinstance(loss, torch.nn.CrossEntropyLoss):\n",
    "                pred_npy = np.argmax(pred_npy, axis=1) \n",
    "                \n",
    "            ###Change the error metric here###\n",
    "\n",
    "            pred_int = np.round(pred_npy).astype(np.uint8).reshape(-1)\n",
    "            target_npy = target_npy.astype(np.uint8).reshape(-1)\n",
    "\n",
    "            _val_accuracies.append(accuracy_score(target_npy, pred_int)*100)\n",
    "\n",
    "        mean_accuracy = sum(_val_accuracies)/len(_val_accuracies)\n",
    "        val_accuracies.append(mean_accuracy)\n",
    "        if lr_schedule:\n",
    "            plot_accuracy(accuracies,val_accuracies, epochs, \"Validation_accuracy_scheduler2%s.png\" % suffix)\n",
    "        else:\n",
    "            plot_accuracy(accuracies,val_accuracies, epochs, \"Validation_accuracy_no_scheduler2%s.png\" % suffix)\n",
    "        print(\"Mean Validation accuracy: %f\" % mean_accuracy)\n",
    "    \n",
    "        \n",
    "        \n",
    "        model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model and Initializing weights\n",
      "Creating sampling weight array\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]C:\\Users\\SpaceMeerkat\\Anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "C:\\Users\\SpaceMeerkat\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:29: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_minimum(a, axis, None, out, keepdims)\n",
      "C:\\Users\\SpaceMeerkat\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:26: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_maximum(a, axis, None, out, keepdims)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:21<00:00, 10.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400\n",
      "Number of labels= 10\n",
      "Weights Created in 21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:20<00:00, 10.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400\n",
      "Number of labels= 10\n",
      "Validation weights Created in 20s\n",
      "Epoch 1 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▍                                                                                 | 1/200 [00:01<06:00,  1.81s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-e724a66fa683>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_schedule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msuffix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TRAIN TIME:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-734b1485f670>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, transforms, data_path, val_path, num_epochs, batch_size, verbose, cube_length, img_size, loss, lr_schedule, initial_lr, suffix)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[0mloss_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m             \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(\"Creating Model and Initializing weights\")\n",
    "\t\n",
    "#    for model_class, loss_fn, suffix in zip([CategoricalNet, RegressionNet], [torch.nn.CrossEntropyLoss(), torch.nn.MSELoss()], [\"_categorical\", \"_regression\"]):\n",
    "#        for schedule in [True, False]:\n",
    "            \n",
    "    model_class, loss_fn, suffix = RegressionNet, torch.nn.CrossEntropyLoss(), \"_categorical\"\n",
    "    schedule = True\n",
    "    \n",
    "    model = model_class()\n",
    "    model.apply(weight_init)\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "start = time.time()\n",
    "train(model, transform, num_epochs=100, batch_size=32, lr_schedule=schedule, loss=loss_fn, suffix=suffix)\n",
    "end = time.time()\n",
    "print('TRAIN TIME:')\n",
    "print('%.2gs'%(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, IMG_PATH+'RegressionNet.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = \"E:/Documents/Python_Scripts/CNN/TRAINING/\"\n",
    "tester = torch.load(IMG_PATH+'RegressionNet.pt').cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in tester.modules():\n",
    "    if isinstance(m, torch.nn.Conv2d):\n",
    "        print(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = fits.open(\"E:/Documents/Python_Scripts/CNN/TRAINING/EXAMPLES/RefL0025N0376,28,12,0,296111.fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()\n",
    "print('TRUE LABEL=',test[0].header['LABEL'])\n",
    "d = test[1].data[200]\n",
    "dat = PIL_transform(d)\n",
    "plt.figure()\n",
    "plt.imshow(dat,cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.savefig(IMG_PATH+'before')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5],[0.5])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = transform(dat.reshape(*dat.shape,1)).unsqueeze(0).float()\n",
    "plt.figure()\n",
    "plt.imshow(data[0,0,:,:],cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.savefig(IMG_PATH+'after')\n",
    "print(data.shape)\n",
    "\n",
    "output = torch.nn.functional.softmax(tester(data),dim=1).detach().numpy()\n",
    "print(output)\n",
    "print('TEST LABEL= ',np.argmax(output))\n",
    "test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = fits.open(\"E:/Documents/Python_Scripts/CNN/TRAINING/EXAMPLES/RefL0100N1504,28,1222,0,9017403.fits\")\n",
    "print('TRUE LABEL=',test2[0].header['LABEL'])\n",
    "d = test2[1].data[30]\n",
    "d = PIL_transform(d)\n",
    "data = transform(d.reshape(*d.shape,1)).unsqueeze(0).float()\n",
    "plt.figure()\n",
    "plt.imshow(data[0,0,:,:],cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "output = torch.nn.functional.softmax(tester(data),dim=1).detach().numpy()\n",
    "print(output)\n",
    "print(np.argmax(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.random.uniform(0,10000,[64,64])\n",
    "d1 = PIL_transform(data1)\n",
    "d1 = transform(d1.reshape(*d1.shape,1)).unsqueeze(0).float() \n",
    "output = torch.nn.functional.softmax(tester(d1),dim=1).detach().numpy()\n",
    "print(output)\n",
    "print(np.argmax(output))\n",
    "plt.figure()\n",
    "plt.imshow(d1[0,0,:,:],cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.uniform(0,10,[64,64])\n",
    "d = PIL_transform(data)\n",
    "d = transform(data.reshape(*data.shape,1)).unsqueeze(0).float()\n",
    "plt.figure()\n",
    "plt.imshow(d[0,0,:,:],cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.savefig(IMG_PATH+'test_image',bbox_inches='tight')\n",
    "output = torch.nn.functional.softmax(tester(d),dim=1).detach().numpy()\n",
    "print(np.argmax(output))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
